---
title: "Ch11_god_spiked_the_integers"
author: "LM"
date: "11/9/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 11.1.1 Logistic regression: Prosocial Chimpanzees

```{r 11.1}

library(rethinking)
data(chimpanzees)
d <- chimpanzees

?chimpanzees

```
 Predictor variables are prosoc_left and condition
Outcome to predict is pulled_left

 four combinations
   1. prosoc_left = 0 and condition = 0: Two food items on the right and no partner
   2. prosoc_let = 1 and condition = 0: Two food items on left and no partner
   3. prosoc_left = 0 and condition = 1: Two food items on right and partner present
   4. prosoc_left = 1 and condition = 1: Two food items on left and partner present
   
```{r 11.2}
# build an index variable contanining the values 1 through 4 to index the combinations above
d$treatment <- 1 + d$prosoc_left + 2*d$condition

```

```{r 11.3}
# verify by using cross-tabs
xtabs( ~ treatment + prosoc_left + condition , d )

```
   The model:
   Li ~ Binomial(1, pi)
   logit(pi) = Aactor + Btreatment
   A ~ tbd
   B ~ tbd
   
This implies 7 A parameters, one for each chimpanzee, and 4 treatment parameters, one for each combination we indexed above.   

Let's consider a runt of a logistic regression, with just a single A parameter in the linear model.
Li ~ Binomial(1, p)
logit(p) = A
A ~ Normal(0,W)

```{r 11.4}

# pick a value for W. Start with something flat

m11.1 <- quap(
  alist(
    pulled_left ~ dbinom(1, p),
    logit(p) <- a,
    a ~ dnorm(0, 1.5 ) #10 )
  ), data = d )
  


```

sample from the prior
```{r 11.5}

set.seed(1999)
prior <- extract.prior(m11.1, n = 1e4 )

```

Convert parameter to outcome scale using the inverse-link function. In this case the link function is logit, so the inverse link is inv_logit.
```{r 11.6}

p <- inv_logit(prior$a)
dens(p, adj = 0.1)

```
For W = 10 most of the probability mass is piled up near 0 or 1, which would mean that the chimpanzees either never or always pull hte left lever. This is silly.

A flat prior in the logit space is not a flat prior in the outcome probability space.

Now we need to determine a prior for the treatment effects B
```{r 11.7}

m11.2 <- quap(
  alist(
    pulled_left ~ dbinom( 1, p ),
    logit(p) <- a + b[treatment],
    a~ dnorm(0, 1.5),
    b[treatment] ~ dnorm(0, 0.5 ) #10)
  ), data = d)

set.seed(1999)
prior <- extract.prior(m11.2, n = 1e4)
p <- sapply(1:4 , function(k) inv_logit(prior$a + prior$b[,k]))
  
```

plot the absolute prior difference between the first two treatments
```{r 11.8}

dens( abs (p[,1] - p[,2] ) , adj = 0.1)

```
   Under Normal(0,10), the model believes, before it sees the data, that the treatments are either completely alike or completely different. That doesn't make sense here.
   Using Normal(0, 0.5), the prior is now concentrated on low absolute differences
   
   Average prior difference is:
```{r 11.9}

m11.3 <- quap(
  alist(
    pulled_left ~ dbinom(1 , p) , 
    logit(p) <- a + b[treatment] , 
    a ~ dnorm(0 , 1.5),
    b[treatment] ~ dnorm(0 , 0.5)
  ), data = d)
set.seed(1999)
prior <- extract.prior(m11.3, n = 1e4)
p <- sapply(1:4, function(k) inv_logit(prior$a + prior$b[,k]))
mean(abs(p[,1] - p[,2] ))
    
```
  about 10%.
  
  Extremely large difference are less plausible. However this is not a strong prior We want our priors to be skeptical of large differences so that we reduce overfitting. Good priors hurt fit to sample but are expected to improve prediction.
  
  
  Finally we have our complete model and are ready to add in all the individual chimpanzee parameters. We'll use Hamiltonian Monte Carlo to approximate the posterior.
  
  First, prepare the data list:
```{r 11.10}

# trimmed data list
dat_list <- list(
pulled_left = d$pulled_left,
actor = d$actor,
treatment = as.integer(d$treatment) )

```
  
Now we can start the Markov chain
```{r 11.11}

m11.4 <- ulam(
alist(
pulled_left ~ dbinom( 1 , p ) ,
logit(p) <- a[actor] + b[treatment] ,
a[actor] ~ dnorm( 0 , 1.5 ),
b[treatment] ~ dnorm( 0 , 0.5 )
) , data=dat_list , chains=4 , log_lik=TRUE ) # this log_lik=TRUE bit tells ulam to compute the calues necessary for PSIS and WAIC
precis( m11.4 , depth=2 )

```
The first 7 parameters are the intercepts unique to each chimpanzee

Let's look at these on the outcome scale:
```{r 11.12}

post <- extract.samples(m11.4)
p_left <- inv_logit( post$a )
plot( precis( as.data.frame(p_left) ) , xlim=c(0,1) )

```
Each row is a chimpanzee, the numbers corresponding to the values in actor. Four of the
individuals—numbers 1, 3, 4, and 5—show a preference for the right lever. Two individuals—
numbers 2 and 7—show the opposite preference.

Now let's consider treatment effects, hopefully estimated more precisely because the model could subtract out the handedness variation among actors. On the logit scale:
```{r 11.13}

labs <- c("R/N","L/N","R/P","L/P")
plot( precis( m11.4 , depth=2 , pars="b" ) , labels=labs )

```
What we are looking for is evidence that the chimpanzees choose the prosocial option more when a partner is present. This implies comparing the first row with the third row and the second row with the fourth row

Calculate the differences between no-partner/partner
```{r 11.14}

diffs <- list(
db13 = post$b[,1] - post$b[,3],  # Prosocial option on the right
db24 = post$b[,2] - post$b[,4] ) # prosocial option on the left
plot( precis(diffs) )

```
db13 shows weak evidence (positive difference but not very large) that individuals pulled left more when the partner was absent, but the compatibility interval is wide. 

for db24, negative differences would be consistent with more prosocial choice when partner is present.

Posterior prediction check
Summarize the proportions of left pulls for each actor in each treatment and plot against the posterior predictions. 
First, to calculate the proportion in each combination of actor and treatment
```{r 11.15}

pl <- by( d$pulled_left , list( d$actor , d$treatment ) , mean )
pl[1,]

# result is a matrix with 7 rows and 4 columns. 
```

Figure 11.4
```{r 11.16}

plot( NULL , xlim=c(1,28) , ylim=c(0,1) , xlab="" ,
ylab="proportion left lever" , xaxt="n" , yaxt="n" )
axis( 2 , at=c(0,0.5,1) , labels=c(0,0.5,1) )
abline( h=0.5 , lty=2 )
for ( j in 1:7 ) abline( v=(j-1)*4+4.5 , lwd=0.5 )
for ( j in 1:7 ) text( (j-1)*4+2.5 , 1.1 , concat("actor ",j) , xpd=TRUE )
for ( j in (1:7)[-2] ) {
lines( (j-1)*4+c(1,3) , pl[j,c(1,3)] , lwd=2 , col=rangi2 )
lines( (j-1)*4+c(2,4) , pl[j,c(2,4)] , lwd=2 , col=rangi2 )
}
points( 1:28 , t(pl) , pch=16 , col="white" , cex=1.7 )
points( 1:28 , t(pl) , pch=c(1,1,16,16) , col=rangi2 , lwd=2 )
yoff <- 0.01
text( 1 , pl[1,1]-yoff , "R/N" , pos=1 , cex=0.8 )
text( 2 , pl[1,2]+yoff , "L/N" , pos=3 , cex=0.8 )
text( 3 , pl[1,3]-yoff , "R/P" , pos=1 , cex=0.8 )
text( 4 , pl[1,4]+yoff , "L/P" , pos=3 , cex=0.8 )
mtext( "observed proportions\n" )

```
Observed data for the chimpanzee data, grouped by actor. Open points are non-partner treatments. Filled points are partner treatments. 

Here's how to compute the posterior predictions
```{r 11.17}

dat <- list( actor=rep(1:7,each=4) , treatment=rep(1:4,times=7) )
p_post <- link( m11.4 , data=dat )  
p_mu <- apply( p_post , 2 , mean )

p_ci <- apply( p_post , 2 , PI )


```
The model expects almost no change when adding a partner. Most of the variation comes from the actor intercepts. Handedness seems to be a bigger predictor than prosocial tendency.

Hypothesis so far has been that the chimpanzees will choose the prosocial option more when the partner is present - that's an interaction effect!

But we could build a model without the interaction and use PSIS or WAIC to compare it to m11.4 and it will probably do just fine, because there is not much evidence of an interaction between the location of the prosocial option and the presence of the partner.

To confirm this guess, here are the new index variables we need:
```{r 11.18}

d$side <- d$prosoc_left + 1 # right 1, left 2
d$cond <- d$condition + 1 # no partner 1, partner 2

```

And here's the model
```{r 11.19}

dat_list2 <- list(
pulled_left = d$pulled_left,
actor = d$actor,
side = d$side,
cond = d$cond )
m11.5 <- ulam(
alist(
pulled_left ~ dbinom( 1 , p ) ,
logit(p) <- a[actor] + bs[side] + bc[cond] ,
a[actor] ~ dnorm( 0 , 1.5 ),
bs[side] ~ dnorm( 0 , 0.5 ),
bc[cond] ~ dnorm( 0 , 0.5 )
) , data=dat_list2 , chains=4 , log_lik=TRUE ) # This again so we can compare with PSIS or WAIC

```

Comparing the two models with PSIS:
```{r 11.20}

 compare( m11.5 , m11.4 , func=PSIS )

```
The model without the interaction is no worse in expected predictive accuracy than the model with it.

OVERTHINKING
```{r 11.21}

post <- extract.samples( m11.4 , clean=FALSE )
str(post)

```

```{r 11.22}

m11.4_stan_code <- stancode(m11.4)
m11.4_stan <- stan( model_code=m11.4_stan_code , data=dat_list , chains=4 )
compare( m11.4_stan , m11.4 )

```
END OVERTHINKING


## 11.1.2 Relative shark and absolute deer

What we did with the chimps was focusing on ABSOLUTE effects, the difference a counter-factual change in a variable might make on an absolute scale of measurement, like the probability of an event.

It is more common to see logistic regressions interpreted through RELATVE effects - proportional changes in the odds of an outcome.If we change a variable
and say the odds of an outcome double, then we are discussing relative effects.

You can calculate these proportional odds relative effect sizes by simply exponentiating the parameter of interest. For example, to calculate the proportional odds of switching from treatment 2 to treatment 4 (adding a partner):
```{r 11.23}

post <- extract.samples(m11.4)
mean( exp(post$b[,4]-post$b[,2]) )

```
On average, the switch multiplies the odds of pulling the left lever by 0.92, an 8% reduction in odds. This is what is meant by proportional odds. The new odds are calculated by taking the old odds and multiplying them by the proportional odds, which is 0.92 in this example.

Relative effects, such as proportional odds, aren't enough to tell us whether a variable is important or not. If the other parameters in the model make the outcome very unlikely, then even a large proportional odds like 5.0 would not make the outcome frequent.

But relative effects are needed to make causal inferences, and can be conditionally very important when other baseline rates change. 

For example, in absolute terms, the lifetime risk of being killed by a deer vastly exceeds the lifetime risk of being killed by a shark. But, conditional on being in the ocean, sharks are much more dangerous than deer. So when we're in the ocean we want to know the relative shark risk.

Neither absolute nor relative risk is sufficient for all purposes. Relative risk can make a mostly irrelevant threat, like death from deer, seem deadly. For general advice, absolute risk often makes more sense. But to make general predictions, conditional on specific circumstances, we still need relative risk. Sharks are absolutely safe, while deer are relatively safe. Both are important truths.

## 11.1.3 Aggregated binomial: Chimpanzees again, condensed. 

IN the last model the data were organized such that each row describes the outcome of a single pull. The models all calculated the likelihood of observing either zero or one pulls of the left-hand lever.

If we don't care about the order of the individual pulls, the same information is contained in a count of how many times each individual pulled the left-hand lever, for each combination of predictor variables.

To calculate the number of times each chimpanzee pulled the left-hand lever, for each combination of predictor values:
```{r 11.24}

data(chimpanzees)
d <- chimpanzees
d$treatment <- 1 + d$prosoc_left + 2*d$condition
d$side <- d$prosoc_left + 1 # right 1, left 2
d$cond <- d$condition + 1 # no partner 1, partner 2
d_aggregated <- aggregate(
d$pulled_left ,
list( treatment=d$treatment , actor=d$actor ,
side=d$side , cond=d$cond ) ,
sum )
colnames(d_aggregated)[5] <- "left_pulls"


```

Now we can get exactly the same inferences as before, just by defining the following model:
```{r 11.25}

dat <- with( d_aggregated , list(
left_pulls = left_pulls,
treatment = treatment,
actor = actor,
side = side,
cond = cond ) )

m11.6 <- ulam(
alist(
left_pulls ~ dbinom( 18 , p ) ,  # There's an 18 here where there use to be a 1
logit(p) <- a[actor] + b[treatment] ,
a[actor] ~ dnorm( 0 , 1.5 ) ,
b[treatment] ~ dnorm( 0 , 0.5 )
) , data=dat , chains=4 , log_lik=TRUE )

```
If we inspect the precis output we see that the posterior distribution is the same as in model m11.4
```{r}

precis(m11.6, depth = 2)
precis(m11.4, depth = 2)

```

However the PSIS (and WAIC) scores are very different between the 0/1 and aggregated models.
```{r 11.26}

 compare( m11.6 , m11.4 , func=PSIS )


```
The major reason for the differences here is that the aggregated model, m11.6, contains an extra factor in its log-probabilities, because of the way the data are organized.

Aggregated probabilities are larger - there are more ways to see the data. So the PSIS/WAIC scores end up being smaller.

A simple example:
```{r 11.27}

# deviance of aggregated 6-in-9
-2*dbinom(6,9,0.2,log=TRUE)
# deviance of dis-aggregated
-2*sum(dbern(c(1,1,1,1,1,1,0,0,0),0.2,log=TRUE))

```
But this difference is entirely meaningless. It's just a side effect of how we organized the data. The posterior distribution for the probability of success on each trial will end up the same either way.

There are two warnings. The first flags the fact that the two models have different numbers of observations. Never compare models fit to different sets of observations!

The other is the Pareto k message at the top. Observations with high Pareto k values are usually influential - the posterior changes a lot when they are dropped from the sample. Because we aggregated by actor-treatment, we forced PSIS to imagine cross-validation taht leaves out all 18 observations in each actor-treatment combination. This makes some observations more influential because they are really now 18 observations.

Bottom line: If you want to calculate WAIC or PSIS, you should use a logistic regression data format, not an aggregated format. Otherwise you are implicitly assuming that only large chunks of the data are separable.

11.1.4 Aggregated binomial: Graduate school admissions

In the example above the number of trials was always 18 on every row. This is often not the case. The way to handle this is to insert a variable from the data in place of the 18. 

An example

Load the data and check it out
```{r 11.28}

library(rethinking)
data(UCBadmit)
d <- UCBadmit

d
```
Each application has a 0 or 1 outcome for admission, but they have been aggregated by department and gender, so there are only 12 rows, representing 4526 applications.

Our job is to evaluate whether these data contain evidence of gender bias in admissions.  We will model the admission decisions, focusing on applicant gender as a predictor variable

```{r 11.29}

dat_list <- list(
admit = d$admit,
applications = d$applications,
gid = ifelse( d$applicant.gender=="male" , 1 , 2 )
)
m11.7 <- ulam(
alist(
admit ~ dbinom( applications , p ) ,
logit(p) <- a[gid] ,
a[gid] ~ dnorm( 0 , 1.5 )
) , data=dat_list , chains=4 )
precis( m11.7 , depth=2 )

```

```{r 11.30}

post <- extract.samples(m11.7)
diff_a <- post$a[,1] - post$a[,2]
diff_p <- inv_logit(post$a[,1]) - inv_logit(post$a[,2])
precis( list( diff_a=diff_a , diff_p=diff_p ) )

```

```{r 11.31}

postcheck( m11.7 )
# draw lines connecting points from same dept
for ( i in 1:6 ) {
x <- 1 + 2*(i-1)
y1 <- d$admit[x]/d$applications[x]
y2 <- d$admit[x+1]/d$applications[x+1]
lines( c(x,x+1) , c(y1,y2) , col=rangi2 , lwd=2 )
text( x+0.5 , (y1+y2)/2 + 0.05 , d$dept[x] , cex=0.8 , col=rangi2 )
}

```

```{r 11.32}

dat_list$dept_id <- rep(1:6,each=2)
m11.8 <- ulam(
alist(
admit ~ dbinom( applications , p ) ,
logit(p) <- a[gid] + delta[dept_id] ,
a[gid] ~ dnorm( 0 , 1.5 ) ,
delta[dept_id] ~ dnorm( 0 , 1.5 )
) , data=dat_list , chains=4 , iter=4000 )
precis( m11.8 , depth=2 )

```

```{r 11.33}

post <- extract.samples(m11.8)
diff_a <- post$a[,1] - post$a[,2]
diff_p <- inv_logit(post$a[,1]) - inv_logit(post$a[,2])
precis( list( diff_a=diff_a , diff_p=diff_p ) )


```

```{r 11.34}

pg <- with( dat_list , sapply( 1:6 , function(k)
applications[dept_id==k]/sum(applications[dept_id==k]) ) )
rownames(pg) <- c("male","female")
colnames(pg) <- unique(d$dept)
round( pg , 2 )

```

```{r 11.35}

y <- rbinom(1e5,1000,1/1000)
c( mean(y) , var(y) )

```

```{r 11.36}

library(rethinking)
data(Kline)
d <- Kline
d

```

```{r 11.37}

d$P <- scale( log(d$population) )
d$contact_id <- ifelse( d$contact=="high" , 2 , 1 )

```

```{r 11.38}

curve( dlnorm( x , 0 , 10 ) , from=0 , to=100 , n=200 )

```

```{r 11.39}

a <- rnorm(1e4,0,10)
lambda <- exp(a)
mean( lambda )

```

```{r 11.40}

curve( dlnorm( x , 3 , 0.5 ) , from=0 , to=100 , n=200 )

```

```{r 11.41}

N <- 100
a <- rnorm( N , 3 , 0.5 )
b <- rnorm( N , 0 , 10 )
plot( NULL , xlim=c(-2,2) , ylim=c(0,100) )
      for ( i in 1:N ) curve( exp( a[i] + b[i]*x ) , add=TRUE , col=grau() )

```

```{r 11.42}

set.seed(10)
N <- 100
a <- rnorm( N , 3 , 0.5 )
b <- rnorm( N , 0 , 0.2 )
plot( NULL , xlim=c(-2,2) , ylim=c(0,100) )
for ( i in 1:N ) curve( exp( a[i] + b[i]*x ) , add=TRUE , col=grau() )


```

```{r 11.43}

x_seq <- seq( from=log(100) , to=log(200000) , length.out=100 )
lambda <- sapply( x_seq , function(x) exp( a + b*x ) )
plot( NULL , xlim=range(x_seq) , ylim=c(0,500) , xlab="log population" ,
ylab="total tools" )
for ( i in 1:N ) lines( x_seq , lambda[i,] , col=grau() , lwd=1.5 )

```

```{r 11.44}

plot( NULL , xlim=range(exp(x_seq)) , ylim=c(0,500) , xlab="population" ,
ylab="total tools" )
for ( i in 1:N ) lines( exp(x_seq) , lambda[i,] , col=grau() , lwd=1.5 )

```

```{r 11.45}

dat <- list(
T = d$total_tools ,
P = d$P ,
cid = d$contact_id )
# intercept only
m11.9 <- ulam(
alist(
T ~ dpois( lambda ),
log(lambda) <- a,
a ~ dnorm( 3 , 0.5 )
), data=dat , chains=4 , log_lik=TRUE )
# interaction model
m11.10 <- ulam(
alist(
T ~ dpois( lambda ),
log(lambda) <- a[cid] + b[cid]*P,
a[cid] ~ dnorm( 3 , 0.5 ),
b[cid] ~ dnorm( 0 , 0.2 )
), data=dat , chains=4 , log_lik=TRUE )

```

```{r 11.46}

compare( m11.9 , m11.10 , func=PSIS )

```

```{r 11.47}

k <- PSIS( m11.10 , pointwise=TRUE )$k
plot( dat$P , dat$T , xlab="log population (std)" , ylab="total tools" ,
col=rangi2 , pch=ifelse( dat$cid==1 , 1 , 16 ) , lwd=2 ,
ylim=c(0,75) , cex=1+normalize(k) )
# set up the horizontal axis values to compute predictions at
ns <- 100
P_seq <- seq( from=-1.4 , to=3 , length.out=ns )
# predictions for cid=1 (low contact)
lambda <- link( m11.10 , data=data.frame( P=P_seq , cid=1 ) )
lmu <- apply( lambda , 2 , mean )
lci <- apply( lambda , 2 , PI )
lines( P_seq , lmu , lty=2 , lwd=1.5 )
shade( lci , P_seq , xpd=TRUE )
# predictions for cid=2 (high contact)
lambda <- link( m11.10 , data=data.frame( P=P_seq , cid=2 ) )
lmu <- apply( lambda , 2 , mean )
lci <- apply( lambda , 2 , PI )
lines( P_seq , lmu , lty=1 , lwd=1.5 )
shade( lci , P_seq , xpd=TRUE )

```

```{r 11.48}

plot( d$population , d$total_tools , xlab="population" , ylab="total tools" ,
col=rangi2 , pch=ifelse( dat$cid==1 , 1 , 16 ) , lwd=2 ,
ylim=c(0,75) , cex=1+normalize(k) )
ns <- 100
P_seq <- seq( from=-5 , to=3 , length.out=ns )
# 1.53 is sd of log(population)
# 9 is mean of log(population)
pop_seq <- exp( P_seq*1.53 + 9 )
lambda <- link( m11.10 , data=data.frame( P=P_seq , cid=1 ) )
lmu <- apply( lambda , 2 , mean )
lci <- apply( lambda , 2 , PI )

```

```{r}

lines( pop_seq , lmu , lty=2 , lwd=1.5 )
shade( lci , pop_seq , xpd=TRUE )
lambda <- link( m11.10 , data=data.frame( P=P_seq , cid=2 ) )
lmu <- apply( lambda , 2 , mean )
lci <- apply( lambda , 2 , PI )
lines( pop_seq , lmu , lty=1 , lwd=1.5 )
shade( lci , pop_seq , xpd=TRUE )

```

```{r 11.49}

dat2 <- list( T=d$total_tools, P=d$population, cid=d$contact_id )
m11.11 <- ulam(
alist(
T ~ dpois( lambda ),
lambda <- exp(a[cid])*P^b[cid]/g,
a[cid] ~ dnorm(1,1),
b[cid] ~ dexp(1),
g ~ dexp(1)
), data=dat2 , chains=4 , log_lik=TRUE )

```


```{r 11.50}

num_days <- 30
y <- rpois( num_days , 1.5 )

```

```{r 11.51}

num_weeks <- 4
y_new <- rpois( num_weeks , 0.5*7 )


```

```{r 11.52}

y_all <- c( y , y_new )
exposure <- c( rep(1,30) , rep(7,4) )
monastery <- c( rep(0,30) , rep(1,4) )
d <- data.frame( y=y_all , days=exposure , monastery=monastery )

```

```{r 11.53}

# compute the offset
d$log_days <- log( d$days )
# fit the model
m11.12 <- quap(
alist(
y ~ dpois( lambda ),
log(lambda) <- log_days + a + b*monastery,
a ~ dnorm( 0 , 1 ),
b ~ dnorm( 0 , 1 )
), data=d )

```

```{r 11.54}

post <- extract.samples( m11.12 )
lambda_old <- exp( post$a )
lambda_new <- exp( post$a + post$b )
precis( data.frame( lambda_old , lambda_new ) )

```

```{r 11.55}

# simulate career choices among 500 individuals
N <- 500 # number of individuals
income <- c(1,2,5) # expected income of each career
score <- 0.5*income # scores for each career, based on income
# next line converts scores to probabilities
p <- softmax(score[1],score[2],score[3])
# now simulate choice
# outcome career holds event type values, not counts
career <- rep(NA,N) # empty vector of choices for each individual
# sample chosen career for each individual
set.seed(34302)
for ( i in 1:N ) career[i] <- sample( 1:3 , size=1 , prob=p )

```

```{r 11.56}

code_m11.13 <- "
data{
int N; // number of individuals
int K; // number of possible careers
int career[N]; // outcome
vector[K] career_income;
}
parameters{
vector[K-1] a; // intercepts
real<lower=0> b; // association of income with choice
}
model{
vector[K] p;
vector[K] s;
a ~ normal( 0 , 1 );
b ~ normal( 0 , 0.5 );
s[1] = a[1] + b*career_income[1];
s[2] = a[2] + b*career_income[2];
s[3] = 0; // pivot
p = softmax( s );
career ~ categorical( p );
}
"

```

```{r 11.57}

dat_list <- list( N=N , K=3 , career=career , career_income=income )
m11.13 <- stan( model_code=code_m11.13 , data=dat_list , chains=4 )
precis( m11.13 , 2 )

```

```{r 11.58}

post <- extract.samples( m11.13 )
# set up logit scores
s1 <- with( post , a[,1] + b*income[1] )
s2_orig <- with( post , a[,2] + b*income[2] )
s2_new <- with( post , a[,2] + b*income[2]*2 )
# compute probabilities for original and counterfactual
p_orig <- sapply( 1:length(post$b) , function(i)
softmax( c(s1[i],s2_orig[i],0) ) )
p_new <- sapply( 1:length(post$b) , function(i)
softmax( c(s1[i],s2_new[i],0) ) )
# summarize
p_diff <- p_new[2,] - p_orig[2,]
precis( p_diff )

```

```{r 11.59}

N <- 500
# simulate family incomes for each individual
family_income <- runif(N)
# assign a unique coefficient for each type of event
b <- c(-2,0,2)
career <- rep(NA,N) # empty vector of choices for each individual
for ( i in 1:N ) {
score <- 0.5*(1:3) + b*family_income[i]
p <- softmax(score[1],score[2],score[3])
career[i] <- sample( 1:3 , size=1 , prob=p 
}
code_m11.14 <- "
data{
int N; // number of observations
int K; // number of outcome values
int career[N]; // outcome
real family_income[N];
}
parameters{
vector[K-1] a; // intercepts
vector[K-1] b; // coefficients on family income
}
model{
vector[K] p;
vector[K] s;
a ~ normal(0,1.5);
b ~ normal(0,1);
for ( i in 1:N ) {
for ( j in 1:(K-1) ) s[j] = a[j] + b[j]*family_income[i];
s[K] = 0; // the pivot
p = softmax( s );
career[i] ~ categorical( p );
}
}
"
dat_list <- list( N=N , K=3 , career=career , family_income=family_income )
m11.14 <- stan( model_code=code_m11.14 , data=dat_list , chains=4 )
precis( m11.14 , 2 )

                     
```

```{r 11.60}

library(rethinking)
data(UCBadmit)
d <- UCBadmit

```

```{r 11.61}

# binomial model of overall admission probability
m_binom <- quap(
alist(
admit ~ dbinom(applications,p),
logit(p) <- a,
a ~ dnorm( 0 , 1.5 )
), data=d )
# Poisson model of overall admission rate and rejection rate
# 'reject' is a reserved word in Stan, cannot use as variable name
dat <- list( admit=d$admit , rej=d$reject )
m_pois <- ulam(
alist(
admit ~ dpois(lambda1),
rej ~ dpois(lambda2),
log(lambda1) <- a1,
log(lambda2) <- a2,
c(a1,a2) ~ dnorm(0,1.5)
), data=dat , chains=3 , cores=3 )

```

```{r 11.62}

inv_logit(coef(m_binom))

```

```{r 11.63}

k <- coef(m_pois)
a1 <- k['a1']; a2 <- k['a2']
exp(a1)/(exp(a1)+exp(a2))

```

























