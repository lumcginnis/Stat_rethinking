---
title: "Ch11_god_spiked_the_integers"
author: "LM"
date: "11/9/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 11.1.1 Logistic regression: Prosocial Chimpanzees

```{r 11.1}

library(rethinking)
data(chimpanzees)
d <- chimpanzees

?chimpanzees

```
 Predictor variables are prosoc_left and condition
Outcome to predict is pulled_left

 four combinations
   1. prosoc_left = 0 and condition = 0: Two food items on the right and no partner
   2. prosoc_let = 1 and condition = 0: Two food items on left and no partner
   3. prosoc_left = 0 and condition = 1: Two food items on right and partner present
   4. prosoc_left = 1 and condition = 1: Two food items on left and partner present
   
```{r 11.2}
# build an index variable contanining the values 1 through 4 to index the combinations above
d$treatment <- 1 + d$prosoc_left + 2*d$condition

```

```{r 11.3}
# verify by using cross-tabs
xtabs( ~ treatment + prosoc_left + condition , d )

```
   The model:
   Li ~ Binomial(1, pi)
   logit(pi) = Aactor + Btreatment
   A ~ tbd
   B ~ tbd
   
This implies 7 A parameters, one for each chimpanzee, and 4 treatment parameters, one for each combination we indexed above.   

Let's consider a runt of a logistic regression, with just a single A parameter in the linear model.
Li ~ Binomial(1, p)
logit(p) = A
A ~ Normal(0,W)

```{r 11.4}

# pick a value for W. Start with something flat

m11.1 <- quap(
  alist(
    pulled_left ~ dbinom(1, p),
    logit(p) <- a,
    a ~ dnorm(0, 1.5 ) #10 )
  ), data = d )
  


```

sample from the prior
```{r 11.5}

set.seed(1999)
prior <- extract.prior(m11.1, n = 1e4 )

```

Convert parameter to outcome scale using the inverse-link function. In this case the link function is logit, so the inverse link is inv_logit.
```{r 11.6}

p <- inv_logit(prior$a)
dens(p, adj = 0.1)

```
For W = 10 most of the probability mass is piled up near 0 or 1, which would mean that the chimpanzees either never or always pull hte left lever. This is silly.

A flat prior in the logit space is not a flat prior in the outcome probability space.

Now we need to determine a prior for the treatment effects B
```{r 11.7}

m11.2 <- quap(
  alist(
    pulled_left ~ dbinom( 1, p ),
    logit(p) <- a + b[treatment],
    a~ dnorm(0, 1.5),
    b[treatment] ~ dnorm(0, 0.5 ) #10)
  ), data = d)

set.seed(1999)
prior <- extract.prior(m11.2, n = 1e4)
p <- sapply(1:4 , function(k) inv_logit(prior$a + prior$b[,k]))
  
```

plot the absolute prior difference between the first two treatments
```{r 11.8}

dens( abs (p[,1] - p[,2] ) , adj = 0.1)

```
   Under Normal(0,10), the model believes, before it sees the data, that the treatments are either completely alike or completely different. That doesn't make sense here.
   Using Normal(0, 0.5), the prior is now concentrated on low absolute differences
   
   Average prior difference is:
```{r 11.9}

m11.3 <- quap(
  alist(
    pulled_left ~ dbinom(1 , p) , 
    logit(p) <- a + b[treatment] , 
    a ~ dnorm(0 , 1.5),
    b[treatment] ~ dnorm(0 , 0.5)
  ), data = d)
set.seed(1999)
prior <- extract.prior(m11.3, n = 1e4)
p <- sapply(1:4, function(k) inv_logit(prior$a + prior$b[,k]))
mean(abs(p[,1] - p[,2] ))
    
```
  about 10%.
  
  Extremely large difference are less plausible. However this is not a strong prior We want our priors to be skeptical of large differences so that we reduce overfitting. Good priors hurt fit to sample but are expected to improve prediction.
  
  
  Finally we have our complete model and are ready to add in all the individual chimpanzee parameters. We'll use Hamiltonian Monte Carlo to approximate the posterior.
  
  First, prepare the data list:
```{r 11.10}

# trimmed data list
dat_list <- list(
pulled_left = d$pulled_left,
actor = d$actor,
treatment = as.integer(d$treatment) )

```
  
Now we can start the Markov chain
```{r 11.11}

m11.4 <- ulam(
alist(
pulled_left ~ dbinom( 1 , p ) ,
logit(p) <- a[actor] + b[treatment] ,
a[actor] ~ dnorm( 0 , 1.5 ),
b[treatment] ~ dnorm( 0 , 0.5 )
) , data=dat_list , chains=4 , log_lik=TRUE ) # this log_lik=TRUE bit tells ulam to compute the calues necessary for PSIS and WAIC
precis( m11.4 , depth=2 )

```
The first 7 parameters are the intercepts unique to each chimpanzee

Let's look at these on the outcome scale:
```{r 11.12}

post <- extract.samples(m11.4)
p_left <- inv_logit( post$a )
plot( precis( as.data.frame(p_left) ) , xlim=c(0,1) )

```
Each row is a chimpanzee, the numbers corresponding to the values in actor. Four of the
individuals—numbers 1, 3, 4, and 5—show a preference for the right lever. Two individuals—
numbers 2 and 7—show the opposite preference.

Now let's consider treatment effects, hopefully estimated more precisely because the model could subtract out the handedness variation among actors. On the logit scale:
```{r 11.13}

labs <- c("R/N","L/N","R/P","L/P")
plot( precis( m11.4 , depth=2 , pars="b" ) , labels=labs )

```
What we are looking for is evidence that the chimpanzees choose the prosocial option more when a partner is present. This implies comparing the first row with the third row and the second row with the fourth row

Calculate the differences between no-partner/partner
```{r 11.14}

diffs <- list(
db13 = post$b[,1] - post$b[,3],  # Prosocial option on the right
db24 = post$b[,2] - post$b[,4] ) # prosocial option on the left
plot( precis(diffs) )

```
db13 shows weak evidence (positive difference but not very large) that individuals pulled left more when the partner was absent, but the compatibility interval is wide. 

for db24, negative differences would be consistent with more prosocial choice when partner is present.

Posterior prediction check
Summarize the proportions of left pulls for each actor in each treatment and plot against the posterior predictions. 
First, to calculate the proportion in each combination of actor and treatment
```{r 11.15}

pl <- by( d$pulled_left , list( d$actor , d$treatment ) , mean )
pl[1,]

# result is a matrix with 7 rows and 4 columns. 
```

Figure 11.4
```{r 11.16}

plot( NULL , xlim=c(1,28) , ylim=c(0,1) , xlab="" ,
ylab="proportion left lever" , xaxt="n" , yaxt="n" )
axis( 2 , at=c(0,0.5,1) , labels=c(0,0.5,1) )
abline( h=0.5 , lty=2 )
for ( j in 1:7 ) abline( v=(j-1)*4+4.5 , lwd=0.5 )
for ( j in 1:7 ) text( (j-1)*4+2.5 , 1.1 , concat("actor ",j) , xpd=TRUE )
for ( j in (1:7)[-2] ) {
lines( (j-1)*4+c(1,3) , pl[j,c(1,3)] , lwd=2 , col=rangi2 )
lines( (j-1)*4+c(2,4) , pl[j,c(2,4)] , lwd=2 , col=rangi2 )
}
points( 1:28 , t(pl) , pch=16 , col="white" , cex=1.7 )
points( 1:28 , t(pl) , pch=c(1,1,16,16) , col=rangi2 , lwd=2 )
yoff <- 0.01
text( 1 , pl[1,1]-yoff , "R/N" , pos=1 , cex=0.8 )
text( 2 , pl[1,2]+yoff , "L/N" , pos=3 , cex=0.8 )
text( 3 , pl[1,3]-yoff , "R/P" , pos=1 , cex=0.8 )
text( 4 , pl[1,4]+yoff , "L/P" , pos=3 , cex=0.8 )
mtext( "observed proportions\n" )

```
Observed data for the chimpanzee data, grouped by actor. Open points are non-partner treatments. Filled points are partner treatments. 

Here's how to compute the posterior predictions
```{r 11.17}

dat <- list( actor=rep(1:7,each=4) , treatment=rep(1:4,times=7) )
p_post <- link( m11.4 , data=dat )  
p_mu <- apply( p_post , 2 , mean )

p_ci <- apply( p_post , 2 , PI )


```
The model expects almost no change when adding a partner. Most of the variation comes from the actor intercepts. Handedness seems to be a bigger predictor than prosocial tendency.

Hypothesis so far has been that the chimpanzees will choose the prosocial option more when the partner is present - that's an interaction effect!

But we could build a model without the interaction and use PSIS or WAIC to compare it to m11.4 and it will probably do just fine, because there is not much evidence of an interaction between the location of the prosocial option and the presence of the partner.

To confirm this guess, here are the new index variables we need:
```{r 11.18}

d$side <- d$prosoc_left + 1 # right 1, left 2
d$cond <- d$condition + 1 # no partner 1, partner 2

```

And here's the model
```{r 11.19}

dat_list2 <- list(
pulled_left = d$pulled_left,
actor = d$actor,
side = d$side,
cond = d$cond )
m11.5 <- ulam(
alist(
pulled_left ~ dbinom( 1 , p ) ,
logit(p) <- a[actor] + bs[side] + bc[cond] ,
a[actor] ~ dnorm( 0 , 1.5 ),
bs[side] ~ dnorm( 0 , 0.5 ),
bc[cond] ~ dnorm( 0 , 0.5 )
) , data=dat_list2 , chains=4 , log_lik=TRUE ) # This again so we can compare with PSIS or WAIC

```

Comparing the two models with PSIS:
```{r 11.20}

 compare( m11.5 , m11.4 , func=PSIS )

```
The model without the interaction is no worse in expected predictive accuracy than the model with it.

OVERTHINKING
```{r 11.21}

post <- extract.samples( m11.4 , clean=FALSE )
str(post)

```

```{r 11.22}

m11.4_stan_code <- stancode(m11.4)
m11.4_stan <- stan( model_code=m11.4_stan_code , data=dat_list , chains=4 )
compare( m11.4_stan , m11.4 )

```
END OVERTHINKING


## 11.1.2 Relative shark and absolute deer

What we did with the chimps was focusing on ABSOLUTE effects, the difference a counter-factual change in a variable might make on an absolute scale of measurement, like the probability of an event.

It is more common to see logistic regressions interpreted through RELATVE effects - proportional changes in the odds of an outcome.If we change a variable
and say the odds of an outcome double, then we are discussing relative effects.

You can calculate these proportional odds relative effect sizes by simply exponentiating the parameter of interest. For example, to calculate the proportional odds of switching from treatment 2 to treatment 4 (adding a partner):
```{r 11.23}

post <- extract.samples(m11.4)
mean( exp(post$b[,4]-post$b[,2]) )

```
On average, the switch multiplies the odds of pulling the left lever by 0.92, an 8% reduction in odds. This is what is meant by proportional odds. The new odds are calculated by taking the old odds and multiplying them by the proportional odds, which is 0.92 in this example.

Relative effects, such as proportional odds, aren't enough to tell us whether a variable is important or not. If the other parameters in the model make the outcome very unlikely, then even a large proportional odds like 5.0 would not make the outcome frequent.

But relative effects are needed to make causal inferences, and can be conditionally very important when other baseline rates change. 

For example, in absolute terms, the lifetime risk of being killed by a deer vastly exceeds the lifetime risk of being killed by a shark. But, conditional on being in the ocean, sharks are much more dangerous than deer. So when we're in the ocean we want to know the relative shark risk.

Neither absolute nor relative risk is sufficient for all purposes. Relative risk can make a mostly irrelevant threat, like death from deer, seem deadly. For general advice, absolute risk often makes more sense. But to make general predictions, conditional on specific circumstances, we still need relative risk. Sharks are absolutely safe, while deer are relatively safe. Both are important truths.

## 11.1.3 Aggregated binomial: Chimpanzees again, condensed. 

IN the last model the data were organized such that each row describes the outcome of a single pull. The models all calculated the likelihood of observing either zero or one pulls of the left-hand lever.

If we don't care about the order of the individual pulls, the same information is contained in a count of how many times each individual pulled the left-hand lever, for each combination of predictor variables.

To calculate the number of times each chimpanzee pulled the left-hand lever, for each combination of predictor values:
```{r 11.24}

data(chimpanzees)
d <- chimpanzees
d$treatment <- 1 + d$prosoc_left + 2*d$condition
d$side <- d$prosoc_left + 1 # right 1, left 2
d$cond <- d$condition + 1 # no partner 1, partner 2
d_aggregated <- aggregate(
d$pulled_left ,
list( treatment=d$treatment , actor=d$actor ,
side=d$side , cond=d$cond ) ,
sum )
colnames(d_aggregated)[5] <- "left_pulls"


```

Now we can get exactly the same inferences as before, just by defining the following model:
```{r 11.25}

dat <- with( d_aggregated , list(
left_pulls = left_pulls,
treatment = treatment,
actor = actor,
side = side,
cond = cond ) )

m11.6 <- ulam(
alist(
left_pulls ~ dbinom( 18 , p ) ,  # There's an 18 here where there use to be a 1
logit(p) <- a[actor] + b[treatment] ,
a[actor] ~ dnorm( 0 , 1.5 ) ,
b[treatment] ~ dnorm( 0 , 0.5 )
) , data=dat , chains=4 , log_lik=TRUE )

```
If we inspect the precis output we see that the posterior distribution is the same as in model m11.4
```{r}

precis(m11.6, depth = 2)
precis(m11.4, depth = 2)

```

However the PSIS (and WAIC) scores are very different between the 0/1 and aggregated models.
```{r 11.26}

 compare( m11.6 , m11.4 , func=PSIS )


```
The major reason for the differences here is that the aggregated model, m11.6, contains an extra factor in its log-probabilities, because of the way the data are organized.

Aggregated probabilities are larger - there are more ways to see the data. So the PSIS/WAIC scores end up being smaller.

A simple example:
```{r 11.27}

# deviance of aggregated 6-in-9
-2*dbinom(6,9,0.2,log=TRUE)
# deviance of dis-aggregated
-2*sum(dbern(c(1,1,1,1,1,1,0,0,0),0.2,log=TRUE))

```
But this difference is entirely meaningless. It's just a side effect of how we organized the data. The posterior distribution for the probability of success on each trial will end up the same either way.

There are two warnings. The first flags the fact that the two models have different numbers of observations. Never compare models fit to different sets of observations!

The other is the Pareto k message at the top. Observations with high Pareto k values are usually influential - the posterior changes a lot when they are dropped from the sample. Because we aggregated by actor-treatment, we forced PSIS to imagine cross-validation taht leaves out all 18 observations in each actor-treatment combination. This makes some observations more influential because they are really now 18 observations.

Bottom line: If you want to calculate WAIC or PSIS, you should use a logistic regression data format, not an aggregated format. Otherwise you are implicitly assuming that only large chunks of the data are separable.

11.1.4 Aggregated binomial: Graduate school admissions

In the example above the number of trials was always 18 on every row. This is often not the case. The way to handle this is to insert a variable from the data in place of the 18. 

An example

Load the data and check it out
```{r 11.28}

library(rethinking)
data(UCBadmit)
d <- UCBadmit

d
```
Each application has a 0 or 1 outcome for admission, but they have been aggregated by department and gender, so there are only 12 rows, representing 4526 applications.

Our job is to evaluate whether these data contain evidence of gender bias in admissions.  We will model the admission decisions, focusing on applicant gender as a predictor variable

We want to fit a binomial regression that models admit as a function of each applicant's gender.
```{r 11.29}

# construct index variable
dat_list <- list(
admit = d$admit,
applications = d$applications,
gid = ifelse( d$applicant.gender=="male" , 1 , 2 ) # gid = gender identity. male = 1, female = 2
)

# the model
m11.7 <- ulam(
alist(
admit ~ dbinom( applications , p ) ,
logit(p) <- a[gid] ,
a[gid] ~ dnorm( 0 , 1.5 )
) , data=dat_list , chains=4 )
precis( m11.7 , depth=2 )

```
The posterior for male applicants, a[1], is higher than that of female applicants.

HOw much higher? Let's calculate the contrast on the logit scale (relative shark) as well as on the outcome scale (absolute deer)
```{r 11.30}

post <- extract.samples(m11.7)
diff_a <- post$a[,1] - post$a[,2]
diff_p <- inv_logit(post$a[,1]) - inv_logit(post$a[,2])
precis( list( diff_a=diff_a , diff_p=diff_p ) )

```
The log-odds difference is positive, corresponding to a higher probability of admission for male applicants. 

On the probability scale itself, the difference is somewhere between 12% and 16%

Posterior check
```{r 11.31}

postcheck( m11.7 ) # default posterior validation check function
# draw lines connecting points from same dept
for ( i in 1:6 ) {
x <- 1 + 2*(i-1)
y1 <- d$admit[x]/d$applications[x]
y2 <- d$admit[x+1]/d$applications[x+1]
lines( c(x,x+1) , c(y1,y2) , col=rangi2 , lwd=2 )
text( x+0.5 , (y1+y2)/2 + 0.05 , d$dept[x] , cex=0.8 , col=rangi2 )
}

```
There are only  departments in which women had a lower rate of admission than men (C and E), yet the model says that women should expect to have a 14% lower chance of admission.

Instead of asking "What are the average probabilities of admission for women and men across all departments?" Let's try asking "What is the average difference in probability of admission between women and men within departments?"

```{r 11.32}

# construct a mumerical index numbering the departments 1-6
dat_list$dept_id <- rep(1:6,each=2) 

# fit the models
m11.8 <- ulam(
alist(
admit ~ dbinom( applications , p ) ,
logit(p) <- a[gid] + delta[dept_id] ,
a[gid] ~ dnorm( 0 , 1.5 ) ,
delta[dept_id] ~ dnorm( 0 , 1.5 )
) , data=dat_list , chains=4 , iter=4000 )

# check posterior
precis( m11.8 , depth=2 )

```
The intercept for male applicants, a[1], is now a little smaller on average than the one for female applicants.

Now let's calculate the contrasts against, both on relative (shark) and absolute (deer) scales:
```{r 11.33}

post <- extract.samples(m11.8)
diff_a <- post$a[,1] - post$a[,2]
diff_p <- inv_logit(post$a[,1]) - inv_logit(post$a[,2])
precis( list( diff_a=diff_a , diff_p=diff_p ) )


```
If male applicants have it worse, it's only by a very small amount, about 2% on average

Why did adding departments to the model change the inference about gender so much?

Rates of admission vary a lot across departments, and women and men applied to different departments.
```{r 11.34}

# Proportions of all applicants in each department that are from either men or women
pg <- with( dat_list , sapply( 1:6 , function(k)
applications[dept_id==k]/sum(applications[dept_id==k]) ) )
rownames(pg) <- c("male","female")
colnames(pg) <- unique(d$dept)
round( pg , 2 )

```
Departments with a larger proportion of women applicants are also those with lower overall admissions rates.

Department is probably a confound, in the sense that it misleads us about the direct causal effect. But it is not a confound, in the sense that it is probably a genuine causal path through gender influences admission. Gender influences choice of department, and department influences chance of admission. Controlling for department reveals a more plausible direct causal influence of gender.

```{r}
postcheck(m11.8)
```
The model lines up much better now with the variation among departments. It infers the direct effect G -> A by conditioning on D and closing the indirect path. This is another example of Mediation Analysis.

What if there are unobserved confounds influencing both department and admissions? Like academic ability, which could influence choice of department and probability of admission. In that case conditioning on department is conditioning on a collider and it opens a non-causal path between gender and admissions.

m11.8 is over-parameterized. We don't actually need both a[1] and a[2], because the individual delta parameters can stand for the acceptance rate of one of the genders in each department. Then we just need an average deviation across departments.

Why might we want to over-parameterize the model? Because it makes it easier to assign priors. If we made one of the genders baseline and measured the other as a deviation from it, we would stumble into the issue of assuming that the acceptance rate for one of the genders is pre-data more uncertain than the other.


### Poisson regression: for when the upper bound isn't known.

The binomial model works here, provided we squint at it the right way. When a binomial distribution has a very small probability of an event p and a very large number of trials N, then it takes on a special shape. The expected value of a binomial distribution is just Np, and its variance is Np(1 − p). But when N is very large and p is very small, then these are approximately the same.

Ex. A monastary is in the business of copying manuscripts. It employs 1000 monks, and each on any particular day about 1 of them finishes a manuscript. Since the monks are working independently of one another, and manuscripts vary in length, some days produce 3 or more manuscripts, and many days produce none. Since this is a binomial process, you can calculate the variance across days as Np(1−p) = 1000(0.001)(1−0.001) ≈ 1. You can simulate this, for example over 10,000 (1e5) days:
```{r 11.35}

y <- rbinom(1e5,1000,1/1000)
c( mean(y) , var(y) )

```
The mean and the variance are nearly identical. This is a special shape of the binomial known as the Poisson distribution. It allows us to model binomial events for which the number of trials N is unknown or uncountably large. 

The conventional link function for a Poisson model is the log link. This ensures that lamda (which represents both mean and variance) is always positive. But it also implies an exponential relationship between predictors and the expected value. Few natural phenomena behave exponentially for long, so one thing to always check with a log link is wheterh it makes sense at all ranges of the predictor variables. Priors on the log scale also scale in surprising ways. So prior predictive simulation is again helpful.

## 11.2.1 Oceanic tool complexity

A number of theories predict that larger populations will both develop and sustain more complex tool kits. It's also suggested that contact rates among populations effectively increase population size, as it's relevant to technological evolution.

Load data: counts of unique tool types for 10 historic Oceanic societies (Locations in Fig 11.6)
```{r 11.36}

library(rethinking)
data(Kline)
d <- Kline
d

```
Keep in mind that the number of rows is not clearly the same as the “sample size” in a count model. The relationship between parameters and “degrees of freedom” is not simple, outside of simple linear regressions.

Still, there isn’t a lot of data here, because there just aren’t that many historic Oceanic societies for which reliable data can be gathered. We’ll want to use regularization to damp down overfitting, as always. But as you’ll see, a lot can still be learned from these data. Any rules you’ve been taught about minimum sample sizes for inference are just non-Bayesian superstitions. 

If you get the prior back, then the data aren’t enough. It’s that simple.

Ok let's try to predict total_tools (the outcome variable).

The impact of population on tool counts is moderated by high contact. This
is to say that the association between total_tools and log population depends
upon contact. So we will look for a positive interaction between log population
and contact rate.

First make new columns with the standardized log of population and an index variable for contact
```{r 11.37}

d$P <- scale( log(d$population) ) # log because theory says that it is the order of magnitude of the population that matters, not the absolute size of it.
d$contact_id <- ifelse( d$contact=="high" , 2 , 1 ) # number of tools increases with the contact rate among islands

```
The model that conforms to the research hypothesis includes an interaction between log-population and contact rate.

First we need to figure out sensible priors. Let's plot a log-normal with these values for the (normal) mean and standard deviation:
```{r 11.38}

curve( dlnorm( x , 0 , 10 ) , from=0 , to=100 , n=200 )

```
This gives us a very large spike around 0, and a very long tail.

How long is the tail
```{r 11.39}

a <- rnorm(1e4,0,10)
lambda <- exp(a)
mean( lambda )

```
Very long. Enough tools to cover an entire island.

McElreath suggests playing around with the curve code, trying different means and standard deviations.

The fact to appreciate is that log link puts half the real numbers (the negative numbers) between 0 and 1 on the outcome scale.

Here is his weakly informative suggestion:
```{r 11.40}

curve( dlnorm( x , 3 , 0.5 ) , from=0 , to=100 , n=200 )

```
Now we need a prior for B, the coefficient of log population.

First we will consider a conventional flat prior like B ~ Normal(0,10)
```{r 11.41}

N <- 100
a <- rnorm( N , 3 , 0.5 )
b <- rnorm( N , 0 , 10 )

# plot 100 prior trends of standardized log population against total tools
plot( NULL , xlim=c(-2,2) , ylim=c(0,100) )
      for ( i in 1:N ) curve( exp( a[i] + b[i]*x ) , add=TRUE , col=grau() )

```
The pivoting around 0 is because that's the average log population. Values on the horizontal axis are z-scores, because the variable is standardized.

This prior think sthat the vast majority of prior relationships between log population and total tools embody either explosive growth just above the mean log population size or rather catastrophic decline right before the mean. This is silly.

Let's dampen the prior's enthusiasm for explosive relationships
```{r 11.42}

set.seed(10)
N <- 100
a <- rnorm( N , 3 , 0.5 )
b <- rnorm( N , 0 , 0.2 ) # much more reasonable
plot( NULL , xlim=c(-2,2) , ylim=c(0,100) )
for ( i in 1:N ) curve( exp( a[i] + b[i]*x ) , add=TRUE , col=grau() )


```
Strong relationships are still possible but most of the mass is for rather flat relationships between total tols and log population.

It will also help to view these priors on more natural outcome scales. The standardized log population variable is good for fitting. But it is bad for thinking. Population size has a natural zero, and we want to keep that in sight. Standardizing the variable destroys that. 

First, here are 100 prior predictive trends between total tools and un-standardized log population:
```{r 11.43}

x_seq <- seq( from=log(100) , to=log(200000) , length.out=100 )
lambda <- sapply( x_seq , function(x) exp( a + b*x ) )
plot( NULL , xlim=range(x_seq) , ylim=c(0,500) , xlab="log population" ,
ylab="total tools" )
for ( i in 1:N ) lines( x_seq , lambda[i,] , col=grau() , lwd=1.5 )

```
The most we expect to ever see in these data is 100 total tools. There are still some explosive options, but not many.

Let's view these same curves on the natural population scale:
```{r 11.44}

plot( NULL , xlim=range(exp(x_seq)) , ylim=c(0,500) , xlab="population" ,
ylab="total tools" )
for ( i in 1:N ) lines( exp(x_seq) , lambda[i,] , col=grau() , lwd=1.5 )

```
Poisson models with log links create log-linear relationships with their predictor variables. When a predictor variable is itself logged, this means we are assuming diminishing returns for the raw variable. Each additional person contributes a smaller increase in the expected number of tools.

Here are some models
```{r 11.45}

dat <- list(
T = d$total_tools ,
P = d$P ,
cid = d$contact_id )

# intercept only
m11.9 <- ulam(
alist(
T ~ dpois( lambda ),
log(lambda) <- a,
a ~ dnorm( 3 , 0.5 )
), data=dat , chains=4 , log_lik=TRUE )

# interaction model
m11.10 <- ulam(
alist(
T ~ dpois( lambda ),
log(lambda) <- a[cid] + b[cid]*P,
a[cid] ~ dnorm( 3 , 0.5 ),
b[cid] ~ dnorm( 0 , 0.2 )
), data=dat , chains=4 , log_lik=TRUE )

```

Take a look at the PSIS comparison
```{r 11.46}

compare( m11.9 , m11.10 , func=PSIS )

```
We get the Pareto k warning again, indicating there are some highly influential points. Unsurprising for a small dataset, but we will need to keep this in mind when looking at the posterior predictions.

It's not surprising that the intercept-only model m11.9 has a worse score, but it might be surprising that the "effective number of parameters" pPSIS is actually larger for the model with fewer parameters. m11.9 has one parameter, and m11.10 has four. WAIC would tell you the same thing. What's happening here?

The only place that model complexity—a model’s tendency to overfit—and parameter count have a clear relationship is in a simple linear regression with flat priors. Once a distribution is bounded, for example, then parameter values near the boundary produce less overfitting than those far from the boundary. The same principle applies to data distributions. Any count near zero is harder to overfit. So overfitting risk depends both upon structural details of the model and the composition of the sample.

In this sample a major source of overfitting risk is the highly influential point flagged by PSIS
Here's the code to plot the data and superimpose posterior predictions for the expected number of tools at each population size and contact rate: (Fig 11.9)
```{r 11.47}

k <- PSIS( m11.10 , pointwise=TRUE )$k
plot( dat$P , dat$T , xlab="log population (std)" , ylab="total tools" ,
col=rangi2 , pch=ifelse( dat$cid==1 , 1 , 16 ) , lwd=2 ,
ylim=c(0,75) , cex=1+normalize(k) )
# set up the horizontal axis values to compute predictions at
ns <- 100
P_seq <- seq( from=-1.4 , to=3 , length.out=ns )
# predictions for cid=1 (low contact)
lambda <- link( m11.10 , data=data.frame( P=P_seq , cid=1 ) )
lmu <- apply( lambda , 2 , mean )
lci <- apply( lambda , 2 , PI )
lines( P_seq , lmu , lty=2 , lwd=1.5 )
shade( lci , P_seq , xpd=TRUE )
# predictions for cid=2 (high contact)
lambda <- link( m11.10 , data=data.frame( P=P_seq , cid=2 ) )
lmu <- apply( lambda , 2 , mean )
lci <- apply( lambda , 2 , PI )
lines( P_seq , lmu , lty=1 , lwd=1.5 )
shade( lci , P_seq , xpd=TRUE )

```

This plot is joined on its right by the same predictions shown on the natural scale, with raw population sizes on the horizontal. The code to do that is very similar, but you need to convert the P_seq to the natural scale, by reversing the standardization, and then you can just replace P_seq with the converted sequence in the lines and shade commands.
```{r 11.48}

plot( d$population , d$total_tools , xlab="population" , ylab="total tools" ,
col=rangi2 , pch=ifelse( dat$cid==1 , 1 , 16 ) , lwd=2 ,
ylim=c(0,75) , cex=1+normalize(k) )
ns <- 100
P_seq <- seq( from=-5 , to=3 , length.out=ns )
# 1.53 is sd of log(population)
# 9 is mean of log(population)
pop_seq <- exp( P_seq*1.53 + 9 )
lambda <- link( m11.10 , data=data.frame( P=P_seq , cid=1 ) )
lmu <- apply( lambda , 2 , mean )
lci <- apply( lambda , 2 , PI )
lines( pop_seq , lmu , lty=2 , lwd=1.5 )
shade( lci , pop_seq , xpd=TRUE )
lambda <- link( m11.10 , data=data.frame( P=P_seq , cid=2 ) )
lmu <- apply( lambda , 2 , mean )
lci <- apply( lambda , 2 , PI )
lines( pop_seq , lmu , lty=1 , lwd=1.5 )
shade( lci , pop_seq , xpd=TRUE )

```
Hawaii, Tonga, and the Trobriand Islands are highly influential points.

Hawaii is very influential. It has extreme population size and the most tools, but low contact. This doesn't mean Hawaii is an outlier that should be dropped, but it does mean that Hawaii strongly influences the posterior distribution. (In the practice problems he has you drop Hawaii and see what changes)

The model can produce a silly pattern, because it lets the intercept be a free parameter. But we know it should be 0. If there are no people, there are no tools.

**OVERTHINKING**

What we want is a dynamic model of the cultural evolution of tools. Tools aren’t created
all at once. Instead they develop over time. Innovation adds them to a population. Processes
of loss remove them. The simplest model assumes that innovation is proportional to population size, but with diminishing returns. Each additional person adds less innovation than
the previous. It also assumes that tool loss is proportional to the number of tools, without
diminishing returns. These forces balance to produce a tool kit of some size.

If m11.10 is the geocentric model, then m11.11 is the scientific model
```{r 11.49}

dat2 <- list( T=d$total_tools, P=d$population, cid=d$contact_id )

m11.11 <- ulam(
alist(
T ~ dpois( lambda ),
lambda <- exp(a[cid])*P^b[cid]/g,
a[cid] ~ dnorm(1,1),
b[cid] ~ dexp(1),
g ~ dexp(1)
), data=dat2 , chains=4 , log_lik=TRUE )

```
**END OVERTHINKING**


## 11.2.2 Negative binomial (gamma-Poisson) models

Typically there is a lot of unexplained variation in Poisson models. Presumably this additional variation arises from unobserved influences that vary from case to case, generating variation in the true λ’s. Ignoring this variation, or rate heterogeneity, can cause confounds just like it can for binomial models.

So a common extension of Poisson GLMs is to swap it for a Negative Binomial distribution (aka Gamma-Poisson distribution), which is really a Poisson distribution in disguise. It is a mixture of different Poisson distribution. 

For the last Poisson example, we’ll look at a case where the exposure varies across observations. When the length of observation, area of sampling, or intensity of sampling varies, the counts we observe also naturally vary. Since a Poisson distribution assumes that the rate of events is constant in time (or space), it’s easy to handle this. All we need to do, as explained above, is to add the logarithm of the exposure to the linear model. The term we add is typically called an offset.

Back to our monastary. Let's simulate a month of daily counts of completed manuscripts:
```{r 11.50}

num_days <- 30
y <- rpois( num_days , 1.5 ) # this is lamda, the rate

# so now y holds 30 days of simulated counts of completed manuscripts

```

Now let's say you're interested in purchasing another monastary. First you want to know how productive the new monastery might be.

The current owners of that monestary keep weekly records, not daily.

Suppose teh daily rate at the new monastery is lambda = 0.5. To simulate data on a weekly basis, we just multiply this average by 7, the exposure:
```{r 11.51}

num_weeks <- 4
y_new <- rpois( num_weeks , 0.5*7 ) # rate * exposure

# y_new holds four weeks of counts of completed manuscripts

```

To analyze both y, totaled up daily, and y_new, totaled up weekly, we just add the logarithm of the exposure to the linear model.

First let's build a data frame to organize the counts and help you see the exposure for each case:
```{r 11.52}

y_all <- c( y , y_new )
exposure <- c( rep(1,30) , rep(7,4) )
monastery <- c( rep(0,30) , rep(1,4) )
d <- data.frame( y=y_all , days=exposure , monastery=monastery )

# d should have three columns: observed counts in y, number of days each count was totaled over in days, and the new monastery is indicated ny monastery 
```

Let's fit the model and estimate hte rate of manuscript production at each monastery. Compute the log of each exposure and then include taht variable in the linear model.
```{r 11.53}

# compute the offset
d$log_days <- log( d$days )

# fit the model
m11.12 <- quap(
alist(
y ~ dpois( lambda ),
log(lambda) <- log_days + a + b*monastery,
a ~ dnorm( 0 , 1 ),
b ~ dnorm( 0 , 1 )
), data=d )

```

Now let's sample from the posterior and then use the linear model, but without the offset. We don't use the offset again when computing predictions because the parameters are already on the daily scale for both monasteries.
```{r 11.54}

post <- extract.samples( m11.12 )
lambda_old <- exp( post$a )
lambda_new <- exp( post$a + post$b )
precis( data.frame( lambda_old , lambda_new ) )

```


### 11.3 Multinomial and categorical models

## 11.3.1 Predictors match to outcomes

Modeling career choice for a number of young adults.
```{r 11.55}

# simulate career choices among 500 individuals
N <- 500 # number of individuals
income <- c(1,2,5) # expected income of each career
score <- 0.5*income # scores for each career, based on income
# next line converts scores to probabilities
p <- softmax(score[1],score[2],score[3])
# now simulate choice
# outcome career holds event type values, not counts
career <- rep(NA,N) # empty vector of choices for each individual
# sample chosen career for each individual
set.seed(34302)
for ( i in 1:N ) career[i] <- sample( 1:3 , size=1 , prob=p )

```

To fit the model to these fake data, we use the dcategorical likelihood, which is the multinomial logistic regression distribution. It works when each calue in the outcome variable, here career, contains the individual event types on each row.
```{r 11.56}

code_m11.13 <- "
data{
int N; // number of individuals
int K; // number of possible careers
int career[N]; // outcome
vector[K] career_income;
}
parameters{
vector[K-1] a; // intercepts
real<lower=0> b; // association of income with choice
}
model{
vector[K] p;
vector[K] s;
a ~ normal( 0 , 1 );
b ~ normal( 0 , 0.5 );
s[1] = a[1] + b*career_income[1];
s[2] = a[2] + b*career_income[2];
s[3] = 0; // pivot
p = softmax( s );  # multinomial logit link
career ~ categorical( p );
}
"

```

Set up the data list and invoke stan:
```{r 11.57}

dat_list <- list( N=N , K=3 , career=career , career_income=income )
m11.13 <- stan( model_code=code_m11.13 , data=dat_list , chains=4 )
precis( m11.13 , 2 )

```

To conduct a counterfactual simulation, we can extract the samples and make our own. The goal is to compare a counterfactual career in which the income is changed. How much does the probability change, in the presence of these competing careers? This is a subtle kind of question, because the probability change always depends upon the other choices. So let’s imagine doubling the income of career 2 above:
```{r 11.58}

post <- extract.samples( m11.13 )
# set up logit scores
s1 <- with( post , a[,1] + b*income[1] )
s2_orig <- with( post , a[,2] + b*income[2] )
s2_new <- with( post , a[,2] + b*income[2]*2 )
# compute probabilities for original and counterfactual
p_orig <- sapply( 1:length(post$b) , function(i)
softmax( c(s1[i],s2_orig[i],0) ) )
p_new <- sapply( 1:length(post$b) , function(i)
softmax( c(s1[i],s2_new[i],0) ) )
# summarize
p_diff <- p_new[2,] - p_orig[2,]
precis( p_diff )

```

## 11.3.2 Predictors matched to observations


Now consider an example in which each observed outcome has unique predictor values. Suppose you are still modeling career choice. But now you want to estimate the association between each person’s family income and which career they choose. So the predictor variable must have the same value in each linear model, for each row in the data. But now there is a unique parameter multiplying it in each linear model. This provides an estimate of the impact of family income on choice, for each type of career.
```{r 11.59}

N <- 500
# simulate family incomes for each individual
family_income <- runif(N)
# assign a unique coefficient for each type of event
b <- c(-2,0,2)
career <- rep(NA,N) # empty vector of choices for each individual
for ( i in 1:N ) {
score <- 0.5*(1:3) + b*family_income[i]
p <- softmax(score[1],score[2],score[3])
career[i] <- sample( 1:3 , size=1 , prob=p 
}
code_m11.14 <- "
data{
int N; // number of observations
int K; // number of outcome values
int career[N]; // outcome
real family_income[N];
}
parameters{
vector[K-1] a; // intercepts
vector[K-1] b; // coefficients on family income
}
model{
vector[K] p;
vector[K] s;
a ~ normal(0,1.5);
b ~ normal(0,1);
for ( i in 1:N ) {
for ( j in 1:(K-1) ) s[j] = a[j] + b[j]*family_income[i];
s[K] = 0; // the pivot
p = softmax( s );
career[i] ~ categorical( p );
}
}
"
dat_list <- list( N=N , K=3 , career=career , family_income=family_income )
m11.14 <- stan( model_code=code_m11.14 , data=dat_list , chains=4 )
precis( m11.14 , 2 )

                     
```

## Multinomial in disguise as Poisson

Back to the Berkely data
```{r 11.60}

library(rethinking)
data(UCBadmit)
d <- UCBadmit

```

Now let’s use a Poisson regression to model both the rate of admission and the rate of rejection. And we’ll compare the inference to the binomial model’s probability of admissi
```{r 11.61}

# binomial model of overall admission probability
m_binom <- quap(
alist(
admit ~ dbinom(applications,p),
logit(p) <- a,
a ~ dnorm( 0 , 1.5 )
), data=d )
# Poisson model of overall admission rate and rejection rate
# 'reject' is a reserved word in Stan, cannot use as variable name
dat <- list( admit=d$admit , rej=d$reject )
m_pois <- ulam(
alist(
admit ~ dpois(lambda1),
rej ~ dpois(lambda2),
log(lambda1) <- a1,
log(lambda2) <- a2,
c(a1,a2) ~ dnorm(0,1.5)
), data=dat , chains=3 , cores=3 )

```

Let’s consider just the posterior means, for the sake of simplicity. But keep in mind that the entire posterior is what matters. First, the inferred binomial probability of admission, across the entire data set, is:
```{r 11.62}

inv_logit(coef(m_binom))

```

And the Poisson model, the implied probability of admission:
```{r 11.63}

k <- coef(m_pois)
a1 <- k['a1']; a2 <- k['a2']
exp(a1)/(exp(a1)+exp(a2))

```

























